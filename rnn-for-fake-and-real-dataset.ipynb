{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\nwordnet_lemmatizer = WordNetLemmatizer()\nstopword = stopwords.words('english')\n\nimport re\ndef my_tokenizer(s):\n    s = s.lower() # downcase\n    s = re.sub(r'@\\S+','',s)\n    s = re.sub(r'[^a-zA-Z0-9\\s]','',s)\n    s = re.sub(r'http\\S+', '', s)\n    tokens = nltk.tokenize.word_tokenize(s) # spliting string into words (tokens)\n    tokens = [t for t in tokens if len(t) > 2] # removing short words, they're probably not useful\n    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] # put words into base form\n    tokens = [t for t in tokens if t not in stopword]\n    return tokens","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real = pd.read_csv('../input/fake-and-real-news-dataset/True.csv',parse_dates = ['date'])\nfake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv',parse_dates = ['date'])\nreal['article'] = real['title']+\" \"+real['text']\nfake['article'] = fake['title']+\" \"+fake['text']\nreal['label'] = 1\nfake['label'] = 0\nall_data = real.append(fake)\nall_data.shape\nmaxLen = len(max(all_data['article'], key=len).split())","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nall_data = all_data.sample(frac = 1)\nall_data.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(44898, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = {'article':[my_tokenizer(sentence) for sentence in all_data['article']],'label':[label for label in all_data['label']]}\ndoc = pd.DataFrame(data)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unseen_test = doc.sample(n = 5000) \ntrain_data = doc.loc[~doc.index.isin(unseen_test.index)]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_vec(article,label,unseen_article,unseen_label):\n    tfidf = TfidfVectorizer(lowercase = False, tokenizer = my_tokenizer,use_idf = True,norm = 'l2',smooth_idf = True)\n    X = tfidf.fit_transform(article)\n    y = label\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50, test_size=0.7, shuffle=True)\n    w_i = tfidf.vocabulary_\n    X_unseen = tfidf.transform(unseen_article)\n    y_unseen = unseen_label\n    return X_train,X_test,y_train,y_test,w_i,X_unseen,y_unseen","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ndef tok_vec(articles,label,unseen_article,unseen_label):\n    article = articles.tolist()\n    labels = label.tolist()\n\n    val_size = int(len(article)*0.9)\n    training_articles = article[0:val_size]\n    testing_articles = article[val_size:]\n    training_labels = labels[0:val_size]\n    testing_labels = labels[val_size:]\n\n    training_labels_final = np.array(training_labels)\n    testing_labels_final = np.array(testing_labels)\n    \n    unseen_article = unseen_article.tolist()\n    unseen_labels = unseen_label.tolist()\n    unseen_labels_final = np.array(unseen_labels)\n    \n    tok = tf.keras.preprocessing.text.Tokenizer(\n        num_words=400000)\n\n    tok.fit_on_texts(training_articles)\n    sequences = tok.texts_to_sequences(training_articles)\n    padded = tf.keras.preprocessing.sequence.pad_sequences(\n        sequences, maxlen=maxLen, dtype='int32', padding='pre', truncating='pre',\n        value=0.0)\n\n    w_i = tok.word_index\n    testing_sequences = tok.texts_to_sequences(testing_articles)\n    testing_padded = tf.keras.preprocessing.sequence.pad_sequences(\n        testing_sequences, maxlen=maxLen, dtype='int32', padding='pre', truncating='pre',\n        value=0.0)\n    unseen_sequences = tok.texts_to_sequences(unseen_article)\n    unseen_padded = tf.keras.preprocessing.sequence.pad_sequences(\n        unseen_sequences, maxlen=maxLen, dtype='int32', padding='pre', truncating='pre',\n        value=0.0)\n    return padded,testing_padded,training_labels_final,testing_labels_final,w_i,unseen_padded,unseen_labels_final","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#choose one of the below vectorizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test,w_i,X_unseen,y_unseen= tfidf_vec(train_data['article'],train_data['label'],\n                                                               unseen_test['article'],unseen_test['label'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test,w_i,X_unseen,y_unseen = tok_vec(train_data['article'],train_data['label'],\n                                                              unseen_test['article'],unseen_test['label'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_embedding_file = '../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt'\n\nprint(\"Loading Glove Model\")\nf = open(word_embedding_file,'r')\nglove_layer = {}\nfor line in f:\n    splitLines = line.split()\n    word = splitLines[0]\n    wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n    glove_layer[word] = wordEmbedding\nprint(\"gloves loaded\")\nvocabLen = len(glove_layer)\nembedding_dim = glove_layer['the'].shape[0]","execution_count":9,"outputs":[{"output_type":"stream","text":"Loading Glove Model\ngloves loaded\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.models import Model\nfrom keras.layers.embeddings import Embedding\n\n\n\nembedding_matrix = np.zeros((vocabLen+1,embedding_dim))\nfor word,idx in w_i.items():\n    embedding_vec = glove_layer.get(word)\n    if embedding_vec is not None:\n        embedding_matrix[idx] = embedding_vec\n        \n\ninput_layer = Input(shape = (maxLen,),dtype = 'int32')\n\nembedded_layer = Embedding(vocabLen+1,embedding_dim,weights = [embedding_matrix],input_length = maxLen,trainable = False)\n\nembedded_sequence = embedded_layer(input_layer)\n\nme = LSTM(512,return_sequences = True)(embedded_sequence)\n\nme = LSTM(512,return_sequences = False)(me)\nme = Dropout(0.6)(me)\nme = Dense(1,activation = 'sigmoid')(me)\nmodel = Model(input_layer,me)\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\nmodel.summary()","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 8148)]            0         \n_________________________________________________________________\nembedding (Embedding)        (None, 8148, 100)         40000100  \n_________________________________________________________________\nlstm (LSTM)                  (None, 8148, 512)         1255424   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 512)               2099200   \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 513       \n=================================================================\nTotal params: 43,355,237\nTrainable params: 3,355,137\nNon-trainable params: 40,000,100\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom tensorflow.keras.utils import plot_model\nplot_model(model,show_shapes = True, show_layer_names = True)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Input\nfrom tensorflow.keras.layers import LSTM,Dense\nfrom tensorflow.keras.models import Model\ndef rnn_model(hp):\n    input_layer = Input(shape = (maxLen,1),dtype = 'float')\n    lay = LSTM(hp.Int('units',min_value=32,max_value=512,step=32))(input_layer)\n    lay = Dense(1,activation = 'softmax')(lay)\n    model = Model(input_layer,lay)\n\n    model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import Input\nfrom tensorflow.keras.layers import LSTM,Dense,Dropout\nfrom tensorflow.keras.models import Model\ninput_layer = Input(shape = (maxLen,1),dtype = 'float')\nlay = LSTM(512,return_sequences = True)(input_layer)\nlay = LSTM(512,return_sequences = False)(lay)\nlay = Dense(1,activation = 'softmax')(lay)\nrnn_model = Model(input_layer,lay)\n\nrnn_model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\nrnn_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import kerastuner as kt\n\ntuner = kt.tuners.Hyperband(rnn_model,objective = 'val_accuracy',max_epochs=10,executions_per_trial = 1,directory='my_dir',project_name = 'fakeandreal')\ntuner.search(X_train,y_train,validation_data = (X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = tuner.get_best_models(1)[0]\ntuner.results_summary()\nbest_hyperparameters = tuner.get_best_hyperparameters(1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmodel.fit(X_train,y_train,validation_data = (X_test,y_test),epochs = 3,batch_size = 32)\n\ntraining_acc = model.evaluate(X_train,y_train)\nval_acc = model.evaluate(X_test,y_test)\ntest_acc = model.evaluate(X_unseen,y_unseen)","execution_count":11,"outputs":[{"output_type":"stream","text":"Epoch 1/3\n1123/1123 [==============================] - 2776s 2s/step - loss: 0.2020 - accuracy: 0.9221 - val_loss: 0.0489 - val_accuracy: 0.9837\nEpoch 2/3\n1123/1123 [==============================] - 2776s 2s/step - loss: 0.1474 - accuracy: 0.9451 - val_loss: 0.2367 - val_accuracy: 0.8977\nEpoch 3/3\n1123/1123 [==============================] - 2766s 2s/step - loss: 0.0382 - accuracy: 0.9884 - val_loss: 0.0060 - val_accuracy: 0.9982\n1123/1123 [==============================] - 878s 782ms/step - loss: 0.0133 - accuracy: 0.9967\n125/125 [==============================] - 97s 776ms/step - loss: 0.0060 - accuracy: 0.9982\n157/157 [==============================] - 122s 777ms/step - loss: 0.0129 - accuracy: 0.9970\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}